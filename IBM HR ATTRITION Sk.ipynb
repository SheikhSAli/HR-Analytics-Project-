{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Analytics Project- Understanding the Attrition in HR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement:**\n",
    "\n",
    "Every year a lot of companies hire a number of employees. The companies invest time and money in training those employees, not just this but there are training programs within the companies for their existing employees as well. The aim of these programs is to increase the effectiveness of their employees. But where HR Analytics fit in this? and is it just about improving the performance of employees?\n",
    "\n",
    "HR Analytics\n",
    "\n",
    "Human resource analytics (HR analytics) is an area in the field of analytics that refers to applying analytic processes to the human resource department of an organization in the hope of improving employee performance and therefore getting a better return on investment. HR analytics does not just deal with gathering data on employee efficiency. Instead, it aims to provide insight into each process by gathering data and then using it to make relevant decisions about how to improve these processes.\n",
    "\n",
    "Attrition in HR\n",
    "\n",
    "Attrition in human resources refers to the gradual loss of employees overtime. In general, relatively high attrition is problematic for companies. HR professionals often assume a leadership role in designing company compensation programs, work culture, and motivation systems that help the organization retain top employees.\n",
    "\n",
    "Attrition affecting Companies\n",
    "\n",
    "A major problem in high employee attrition is its cost to an organization. Job postings, hiring processes, paperwork, and new hire training are some of the common expenses of losing employees and replacing them. Additionally, regular employee turnover prohibits your organization from increasing its collective knowledge base and experience over time. This is especially concerning if your business is customer-facing, as customers often prefer to interact with familiar people. Errors and issues are more likely if you constantly have new workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Attrition affect companies? and how does HR Analytics help in analyzing attrition? We will write the code and try to understand the process step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv('Attrition_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the dataset to obtain - Count , Mean , Standard deviation , Mininmum , IQR , Maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting *Countplot* against the target variable.\n",
    "\n",
    "A count plot can be thought of as a histogram across a **categorical** , instead of quantitative, variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "sns.countplot('Age', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "sns.countplot('BusinessTravel', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "sns.countplot('Department', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "sns.countplot('EducationField', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "sns.countplot('Gender', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,9))\n",
    "sns.countplot('JobRole', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,9))\n",
    "sns.countplot('MaritalStatus', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,9))\n",
    "sns.countplot('OverTime', hue='Attrition', data=adf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our target variable is Attrition and other are our predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset there are twp type of datatypes - **Object** and **Integer**  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding datas with **Object** type ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type = [feature for feature in adf.columns if adf[feature].dtypes =='O']\n",
    "print(object_type)\n",
    "print(\"Number of columns with object data type in adf is :\" , len(object_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns found above are of **Object** datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding datas with **Integer** datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_type = [feature for feature in adf.columns if adf[feature].dtypes !='O']\n",
    "print(integer_type)\n",
    "print(\"Number of columns with int64 data type is : \" , len(integer_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can observe that\n",
    "9 columns are as object datatype and 25 columns are as integer datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Values of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = ['Attrition', 'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime']\n",
    "for col in object_col:\n",
    "    print(col,'\\n',adf[col].value_counts() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use **LabelEncoder** and it will also produce the same result , But here I have used .replace method to assign values explicitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attrition**\n",
    "\n",
    "Attrition is our target variable , and has two categories so we'll transfrom its categories (Yes & No) to 1 & 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.Attrition.replace({'Yes': 1, 'No': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Travel**\n",
    "\n",
    "This column has three values \n",
    "\n",
    "1.**Non-Travel**\n",
    "\n",
    "2.**Travel-Rarely**\n",
    "\n",
    "3.**Travel-Frequently**\n",
    "\n",
    "so we will replace them by 0 , 1 , 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.BusinessTravel.replace({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently':2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Department**\n",
    "\n",
    "This column has three values \n",
    "\n",
    "1.**Sales**\n",
    "\n",
    "2.**Research & Development**\n",
    "\n",
    "\n",
    "3.**Human Resources**\n",
    "\n",
    "So we will convert them with 0 , 1 , 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.Department.replace({'Sales': 0, 'Research & Development': 1, 'Human Resources': 2, }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EducationField**\n",
    "\n",
    "This column has 6 values\n",
    "\n",
    "1.**Life Sciences**   \n",
    "\n",
    "2.**Medical**    \n",
    "\n",
    "3.**Marketing** \n",
    "\n",
    "4.**Technical Degree**\n",
    "\n",
    "5.**Human Resources**\n",
    "\n",
    "6.**Other** \n",
    "\n",
    "So we will convert these values with 0 , 1 , 2 , 3 , 4 , 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.EducationField.replace({'Life Sciences': 0, 'Medical': 1, 'Marketing': 2, \n",
    "                             'Technical Degree': 3, 'Human Resources': 4, 'Other':5},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['EducationField'].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender**\n",
    "\n",
    "This column has 2 values \n",
    "\n",
    "1.**Male**\n",
    "\n",
    "2.**Female**\n",
    "\n",
    "So we will convert them by 1 , 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.Gender.replace({'Male': 1, 'Female': 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['Gender'].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JobRole**\n",
    "\n",
    "This column has nine values\n",
    "\n",
    "1.**Sales Executive**\n",
    "\n",
    "2.**Research Scientist**\n",
    "\n",
    "3.**Laboratory Technician**\n",
    "\n",
    "4.**Manufacturing Director**\n",
    "\n",
    "5.**Healthcare Representative**\n",
    "\n",
    "6.**Manager**\n",
    "\n",
    "7.**Sales Representative**\n",
    "\n",
    "8.**Research Director**\n",
    "\n",
    "9.**Human Resources**\n",
    "\n",
    "So we will convert them by 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.JobRole.replace({'Sales Executive': 0, 'Research Scientist': 1, 'Laboratory Technician': 2,\n",
    "                     'Manufacturing Director': 3, 'Healthcare Representative': 4, 'Manager': 5,\n",
    "                     'Sales Representative': 6, 'Research Director': 7, 'Human Resources': 8}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['JobRole']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MaritalStatus**\n",
    "\n",
    "This column has 3 values\n",
    "\n",
    "1.**Single**\n",
    "\n",
    "2.**Married**\n",
    "\n",
    "3.**Divorced**\n",
    "\n",
    "So we will convert them by 0 , 1 , 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " adf.MaritalStatus.replace({'Single': 0, 'Married': 1, 'Divorced': 2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['MaritalStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Over18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['Over18'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all employees are over 18 , so we can droop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.drop('Over18' , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverTime**\n",
    "\n",
    "This colummn has two values \n",
    "\n",
    "1.**Yes**\n",
    "\n",
    "2.**No**\n",
    "\n",
    "So we will convert it by 1 , 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.OverTime.replace({'Yes': 1, 'No': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['OverTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for object values in the dataset if any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type = [feature for feature in adf.columns if adf[feature].dtypes =='O']\n",
    "print(object_type)\n",
    "print(\"Number of columns with object data type in adf is :\" , len(object_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sucessfully converted all our **Object** datatype columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualising Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Boxplot*\n",
    "\n",
    "A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist=adf.columns.values\n",
    "ncol=8\n",
    "nrows=5\n",
    "plt.figure(figsize=(16, 9 ))\n",
    "for i in range (0,len(collist)):\n",
    "    plt.subplot(nrows,ncol,i+1)\n",
    "    sns.boxplot(adf[collist[i]],color='darkcyan',orient='h')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some of the columns has Outliers , so we need to deal with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can observe there are Outliers in our dataset , So we need to remove these outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Zscore of our datas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A  **Z-score** is a numerical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean. If a Z-score is 0, it indicates that the data point's score is identical to the mean score. A Z-score of 1.0 would indicate a value that is one standard deviation from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score = np.abs(zscore(adf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(z_score>3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above arrays we can observe that first array shows row number and second array shows column number of values having Zscore > 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adf_wo = Our DataFrame without Outliers ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_wo = adf.drop([  28,   45,   62,   62,   63,   64,   85,   98,   98,  110,  123,\n",
    "        123,  123,  126,  126,  126,  153,  178,  187,  187,  190,  190,\n",
    "        218,  231,  231,  237,  237,  270,  270,  281,  326,  386,  386,\n",
    "        401,  411,  425,  425,  427,  445,  466,  473,  477,  535,  561,\n",
    "        561,  584,  592,  595,  595,  595,  616,  624,  635,  653,  653,\n",
    "        677,  686,  701,  716,  746,  749,  752,  799,  838,  861,  861,\n",
    "        875,  875,  894,  914,  914,  918,  922,  926,  926,  937,  956,\n",
    "        962,  976,  976, 1008, 1024, 1043, 1078, 1078, 1086, 1086, 1093,\n",
    "       1111, 1116, 1116, 1135, 1138, 1138, 1156, 1184, 1221, 1223, 1242,\n",
    "       1295, 1301, 1301, 1303, 1327, 1331, 1348, 1351, 1401, 1414, 1430])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here above we have successfully removed all the rows having outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnss = ['Attrition','Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EmployeeCount', 'EmployeeNumber', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Correlation between our predictor variable and target variable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (24,12))\n",
    "sns.heatmap(adf_wo.corr()  , cmap = 'YlGnBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = adf_wo.corr()\n",
    "corr_df  = corr_df.iloc[: , 1:2]\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can observe that some columns are internally correlated ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Skewness in our dataset[features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predictor = adf_wo.drop('Attrition', axis = 1)\n",
    "x_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predictor.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DistPlot*\n",
    "\n",
    "This function combines the matplotlib hist function (with automatic calculation of a good default bin size) with the seaborn kdeplot() and rugplot() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in x_predictor :\n",
    "    sns.distplot(x_predictor[feature] , kde = True , color = 'darkcyan' )\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule of thumb seems to be: If the skewness is between -0.5 and 0.5, the data are fairly symmetrical. If the skewness is between -1 and – 0.5 or between 0.5 and 1, the data are moderately skewed. If the skewness is less than -1 or greater than 1, the data are highly skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powert = PowerTransformer( method = 'yeo-johnson' , standardize = False)\n",
    "x_t = powert.fit_transform(x_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans = pd.DataFrame(x_t , columns = x_predictor.columns)\n",
    "x_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in x_predictor :\n",
    "    sns.distplot(x_predictor[feature] , kde = True , color = 'darkcyan' )\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we have successfully removed skewness of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for min and max values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_trans :\n",
    "    print(i , max(x_trans[i]) - min(x_trans[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is vast different between values of different columns , So we will scale them ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian's distribution with zero mean and unit variance is standard scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_s = scaler.fit_transform(x_trans)\n",
    "x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sc = pd.DataFrame(x_s , columns = x_trans.columns)\n",
    "x_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can drop **EmployeeNumber** column as its not worth taking forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sc = x_sc.drop('EmployeeNumber' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,9))\n",
    "sns.heatmap(x_sc.corr() , cmap = 'Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above heatmap we can observe that some **predictor columns** are highly correlated . So we will use PCA for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 'mle' , svd_solver = 'full' )\n",
    "xpca = pca.fit_transform(x_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_f = pd.DataFrame(xpca )\n",
    "x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can observe that the columns are reduced to 28 from 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adf_wo.iloc[: ,1:2 ]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding best Random State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(0,200):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=i)\n",
    "    rf=RandomForestClassifier()\n",
    "    rf.fit(X_train,y_train)\n",
    "    predrf=rf.predict(X_test)\n",
    "    acc=accuracy_score(y_test,predrf)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "    \n",
    "print('Best accuracy is ',maxAccu, 'on random state ',maxRS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model, x_train, y):\n",
    "    rmse =- (cross_val_score(model, x_train, y, scoring='neg_mean_squared_error', cv=5))\n",
    "    return(rmse*100)\n",
    "\n",
    "models = [LogisticRegression(),\n",
    "             KNeighborsClassifier(),\n",
    "             SVC(),\n",
    "             RandomForestClassifier(),\n",
    "             AdaBoostClassifier(),\n",
    "             DecisionTreeClassifier(),\n",
    "             GaussianNB()\n",
    "         ]\n",
    "\n",
    "names = ['LogisticRegression','K Nearest Neighbor','Support Vector Classifier','Random Forest','AdaBoost Classifier',\n",
    "         'Decision Tree Classifier' , 'GaussianNB' ]\n",
    "\n",
    "for model,name in zip(models,names):\n",
    "    fit = model.fit(X_train , y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    score = model.score(X_train , y_train)\n",
    "    print(name ,\" - \" ,score)\n",
    "    print(\"Accuracy Score\" , accuracy_score(y_test , y_predicted))\n",
    "    print(\"Confusion Matrix\" , confusion_matrix(y_test , y_predicted))\n",
    "    print(\"Classification Report\" , classification_report(y_test , y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can conclude that **Support Vector Classifier ,  AdaBoost Classifier , Random Forest Classifier and Decision Tree Classifier** have the best scores so we will use these algorithms for our future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(0,200):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=i)\n",
    "    sv = SVC()\n",
    "    sv.fit(X_train,y_train)\n",
    "    predsv = sv.predict(X_test)\n",
    "    acc=accuracy_score(y_test,predsv)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "    \n",
    "print('Best accuracy is ',maxAccu, 'on random state ',maxRS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=99)\n",
    "sv = SVC()\n",
    "sv.fit(X_train , y_train)\n",
    "sv_predicted = sv.predict(X_test)\n",
    "score = sv.score(X_train , y_train)\n",
    "\n",
    "print(SVC() ,\" - \" ,score)\n",
    "print(\"Accuracy:\",accuracy_score(sv_predicted, y_test))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(sv_predicted, y_test))\n",
    "print(\"\\t\\tclassification report\")\n",
    "print(\"-\" * 52)\n",
    "print(classification_report(sv_predicted , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(0,200):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=i)\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train,y_train)\n",
    "    predrf = rf.predict(X_test)\n",
    "    acc=accuracy_score(y_test,predrf)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "    \n",
    "print('Best accuracy is ',maxAccu, 'on random state ',maxRS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=106)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train , y_train)\n",
    "rf_predicted = rf.predict(X_test)\n",
    "score = rf.score(X_train , y_train)\n",
    "\n",
    "print(RandomForestClassifier() ,\" - \" ,score)\n",
    "print(\"Accuracy:\",accuracy_score(rf_predicted, y_test))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(rf_predicted, y_test))\n",
    "print(\"\\t\\tclassification report\")\n",
    "print(\"-\" * 52)\n",
    "print(classification_report(rf_predicted , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(0,200):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=i)\n",
    "    dtr = DecisionTreeClassifier()\n",
    "    dtr.fit(X_train,y_train)\n",
    "    preddtr = dtr.predict(X_test)\n",
    "    acc=accuracy_score(y_test,preddtr)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "    \n",
    "print('Best accuracy is ',maxAccu, 'on random state ',maxRS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=106)\n",
    "dtr = DecisionTreeClassifier()\n",
    "dtr.fit(X_train , y_train)\n",
    "dtr_predicted = dtr.predict(X_test)\n",
    "score = dtr.score(X_train , y_train)\n",
    "\n",
    "print(DecisionTreeClassifier() ,\" - \" ,score)\n",
    "print(\"Accuracy:\",accuracy_score(dtr_predicted, y_test))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(dtr_predicted, y_test))\n",
    "print(\"\\t\\tclassification report\")\n",
    "print(\"-\" * 52)\n",
    "print(classification_report(dtr_predicted , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(0,200):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=i)\n",
    "    ab = AdaBoostClassifier()\n",
    "    ab.fit(X_train,y_train)\n",
    "    predab = ab.predict(X_test)\n",
    "    acc=accuracy_score(y_test,predab)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "    \n",
    "print('Best accuracy is ',maxAccu, 'on random state ',maxRS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=166)\n",
    "ab = AdaBoostClassifier()\n",
    "ab.fit(X_train , y_train)\n",
    "ab_predicted = ab.predict(X_test)\n",
    "score = ab.score(X_train , y_train)\n",
    "\n",
    "print(AdaBoostClassifier() ,\" - \" ,score)\n",
    "print(\"Accuracy:\",accuracy_score(ab_predicted, y_test))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(ab_predicted, y_test))\n",
    "print(\"\\t\\tclassification report\")\n",
    "print(\"-\" * 52)\n",
    "print(classification_report(ab_predicted , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost Classifier **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate':[0.001, 0.10, 0.1, 1],\n",
    "\n",
    "             'n_estimators':range(50, 400, 50)\n",
    "             }\n",
    "\n",
    "\n",
    "ab = AdaBoostClassifier( random_state = 166)\n",
    "\n",
    "\n",
    "grid_ab = GridSearchCV(ab , param_grid, scoring = 'accuracy')\n",
    "grid_ab.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyper Parameters:\\n\",grid_ab.best_params_)\n",
    "print(\"training accuracy:\\n\",grid_ab.best_score_)\n",
    "ab_grid_pred = grid_ab.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(ab_grid_pred , y_test))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(ab_grid_pred , y_test))\n",
    "print(\"\\t\\tclassification report\")\n",
    "print(\"-\" * 52)\n",
    "print(classification_report(ab_grid_pred , y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can observe that **Random Forest Classifier** is giving the best scores for our Predictions .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Random Forest Classifier again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(x_f,y,test_size=.30,random_state=106)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\n",
    "random_forest.fit(X_train, y_train)\n",
    "Y_prediction = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n",
    "print(round(acc_random_forest,2,), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n",
    "              \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n",
    "              \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n",
    "              \"n_estimators\": [100, 400, 700, 1000, 1500]}\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(random_forest, X_train, y_train, cv=3)\n",
    "confusion_matrix(Y_train, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision and Recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and Recall:\n",
    "print(\"Precision:\", precision_score(Y_train, predictions))\n",
    "print(\"Recall:\",recall_score(Y_train, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the Probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = random_forest.predict_proba(X_train)\n",
    "y_scores = y_scores[:,1]\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(y_train, y_scores)\n",
    "def plot_precision_and_recall(precision, recall, threshold):\n",
    "    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n",
    "    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n",
    "    plt.xlabel(\"threshold\", fontsize=19)\n",
    "    plt.legend(loc=\"upper right\", fontsize=19)\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plot_precision_and_recall(precision, recall, threshold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC_AUC Curve**\n",
    "\n",
    "This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# plotting them against each other\n",
    "\n",
    "def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n",
    "    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plot_roc_curve(false_positive_rate, true_positive_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(rf,'RandomForestClassifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
